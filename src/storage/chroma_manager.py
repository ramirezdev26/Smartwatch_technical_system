"""
ChromaManager - Maneja todas las operaciones con Chroma DB
Fase 1: Almacenamiento e integraci√≥n con el pipeline existente
"""
import chromadb
from typing import List, Dict, Any, Optional
from loguru import logger
import uuid
import time
from pathlib import Path


class ChromaManager:
    """Gestor de base de datos vectorial Chroma para smartwatch docs"""

    def __init__(self, host: str = "localhost", port: int = 8000, collection_name: str = "smartwatch_docs"):
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.client = None
        self.collection = None

        # Conectar autom√°ticamente
        self.connect()

    def connect(self):
        """Establece conexi√≥n con Chroma DB"""
        try:
            logger.info(f"üîå Conectando a Chroma DB en {self.host}:{self.port}")

            # Crear cliente HTTP
            self.client = chromadb.HttpClient(host=self.host, port=self.port)

            # Verificar conexi√≥n
            heartbeat = self.client.heartbeat()
            logger.info(f"‚úÖ Conexi√≥n exitosa a Chroma DB: {heartbeat}")

            # Crear o obtener colecci√≥n
            self._setup_collection()

        except Exception as e:
            logger.error(f"‚ùå Error conectando a Chroma DB: {e}")
            logger.error("üí° Aseg√∫rate de que el contenedor Docker est√© corriendo:")
            logger.error("   docker run -v ./chroma-data:/data -p 8000:8000 chromadb/chroma")
            raise

    def _setup_collection(self):
        """Configura la colecci√≥n de documentos"""
        try:
            # Intentar obtener colecci√≥n existente
            self.collection = self.client.get_collection(name=self.collection_name)

            # Verificar contenido existente
            count = self.collection.count()
            logger.info(f"üìö Colecci√≥n '{self.collection_name}' encontrada con {count} documentos")

        except Exception:
            # Crear nueva colecci√≥n si no existe
            logger.info(f"üÜï Creando nueva colecci√≥n '{self.collection_name}'")

            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "Smartwatch technical documentation embeddings",
                    "embedding_model": "all-MiniLM-L6-v1",
                    "embedding_dimension": 384,
                    "created_by": "smartwatch_knowledge_system"
                }
            )
            logger.info(f"‚úÖ Colecci√≥n '{self.collection_name}' creada exitosamente")

    def store_documents(self, processed_docs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Almacena documentos procesados en Chroma

        Args:
            processed_docs: Lista de documentos procesados con chunks y embeddings

        Returns:
            Estad√≠sticas del almacenamiento
        """
        logger.info("üíæ Almacenando documentos en Chroma DB...")
        start_time = time.time()

        all_ids = []
        all_embeddings = []
        all_documents = []
        all_metadatas = []

        total_chunks = 0

        for doc in processed_docs:
            doc_metadata = doc["metadata"]
            chunks = doc["chunks"]

            logger.info(f"üìÑ Procesando {doc_metadata['file_name']}: {len(chunks)} chunks")

            for i, chunk in enumerate(chunks):
                # Crear ID √∫nico
                chunk_id = self._generate_chunk_id(doc_metadata, i)

                # Preparar metadatos enriquecidos
                enriched_metadata = {
                    "brand": doc_metadata.get("brand", "unknown"),
                    "document_name": doc_metadata["file_name"],
                    "document_path": doc_metadata["file_path"],
                    "chunk_index": i,
                    "word_count": chunk.get("word_count", 0),
                    "char_count": chunk.get("char_count", 0),
                    "document_type": doc_metadata.get("document_type", "pdf"),
                    "processing_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "total_chunks_in_doc": len(chunks),
                    "embedding_model": chunk.get("embedding_model", "all-MiniLM-L6-v1"),
                    "embedding_dimension": len(chunk["embedding"])
                }
                if "quality_analysis" in doc_metadata:
                    enriched_metadata["document_quality"] = doc_metadata["quality_analysis"]["document_quality"]

                chunk_quality = None

                # Buscar calidad del chunk en diferentes campos (por compatibilidad)
                if "chunk_quality" in chunk:
                    chunk_quality = chunk["chunk_quality"]
                elif "quality_label" in chunk:
                    chunk_quality = chunk["quality_label"]
                elif "auto_label" in chunk:
                    chunk_quality = chunk["auto_label"]

                # Guardar calidad del chunk si existe
                if chunk_quality:
                    enriched_metadata["chunk_quality"] = chunk_quality
                    enriched_metadata["quality_confidence"] = chunk.get("quality_confidence",
                                                                        chunk.get("auto_confidence", 0.0))

                # Agregar a listas
                all_ids.append(chunk_id)
                all_embeddings.append(chunk["embedding"])
                all_documents.append(chunk["text"])
                all_metadatas.append(enriched_metadata)

                total_chunks += 1

        # Almacenar en lotes para eficiencia
        batch_size = 100  # Chroma recomienda lotes de ~100
        stored_count = 0

        for i in range(0, len(all_ids), batch_size):
            batch_end = min(i + batch_size, len(all_ids))

            batch_ids = all_ids[i:batch_end]
            batch_embeddings = all_embeddings[i:batch_end]
            batch_documents = all_documents[i:batch_end]
            batch_metadatas = all_metadatas[i:batch_end]

            try:
                self.collection.add(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas
                )

                stored_count += len(batch_ids)
                logger.info(f"   üíæ Lote {i // batch_size + 1}: {len(batch_ids)} chunks almacenados")

            except Exception as e:
                logger.error(f"‚ùå Error almacenando lote {i // batch_size + 1}: {e}")
                raise

        storage_time = time.time() - start_time

        # Verificar almacenamiento
        final_count = self.collection.count()

        logger.info(f"‚úÖ Almacenamiento completado:")
        logger.info(f"   üì¶ Chunks almacenados: {stored_count}")
        logger.info(f"   üìö Total en colecci√≥n: {final_count}")
        logger.info(f"   ‚è±Ô∏è Tiempo: {storage_time:.2f} segundos")
        logger.info(f"   ‚ö° Velocidad: {stored_count / storage_time:.1f} chunks/segundo")

        return {
            "chunks_stored": stored_count,
            "total_in_collection": final_count,
            "storage_time": storage_time,
            "storage_rate": stored_count / storage_time,
            "success": True
        }

    def _generate_chunk_id(self, doc_metadata: Dict[str, Any], chunk_index: int) -> str:
        """Genera ID √∫nico para un chunk"""
        brand = doc_metadata.get("brand", "unknown")
        doc_name = Path(doc_metadata["file_name"]).stem  # Sin extensi√≥n

        # Crear ID legible y √∫nico
        chunk_id = f"{brand}_{doc_name}_chunk_{chunk_index:03d}"

        # Limpiar caracteres problem√°ticos
        chunk_id = chunk_id.replace(" ", "_").replace("-", "_").lower()

        return chunk_id

    def search(self, query: str, top_k: int = 5, brand_filter: str = None, prioritize_relevant: bool = True) -> List[
        Dict[str, Any]]:
        """
        Realiza b√∫squeda sem√°ntica en Chroma con priorizaci√≥n de chunks relevantes

        Args:
            query: Consulta en lenguaje natural
            top_k: N√∫mero de resultados a retornar
            brand_filter: Filtrar por marca espec√≠fica
            prioritize_relevant: Si priorizar chunks marcados como "relevante"

        Returns:
            Lista de resultados con metadatos y scores, priorizados por relevancia
        """
        if not self.collection:
            raise ValueError("‚ùå No hay conexi√≥n a Chroma DB")

        logger.info(f"üîç Buscando en Chroma: '{query}' (prioridad: {'relevantes' if prioritize_relevant else 'todos'})")
        start_time = time.time()

        # Preparar filtros base
        base_filter = {}
        if brand_filter:
            base_filter["brand"] = brand_filter.lower()

        processed_results = []

        try:
            if prioritize_relevant:
                # PASO 1: Buscar primero en chunks RELEVANTES
                relevant_filter = base_filter.copy()
                relevant_filter["chunk_quality"] = "relevante"

                logger.info("üéØ Buscando primero en chunks RELEVANTES...")
                relevant_results = self.collection.query(
                    query_texts=[query],
                    n_results=min(top_k * 2, 20),  # Buscar m√°s para tener opciones
                    where=relevant_filter if relevant_filter else None,
                    include=["documents", "metadatas", "distances"]
                )

                # Procesar resultados relevantes
                if relevant_results["documents"] and relevant_results["documents"][0]:
                    for i in range(len(relevant_results["documents"][0])):
                        result = {
                            "text": relevant_results["documents"][0][i],
                            "metadata": relevant_results["metadatas"][0][i],
                            "distance": relevant_results["distances"][0][i],
                            "similarity_score": 1 - relevant_results["distances"][0][i],
                            "ranking": len(processed_results) + 1,
                            "priority": "üéØ RELEVANTE"
                        }
                        processed_results.append(result)

                # PASO 2: Si necesitamos m√°s resultados, buscar en el resto
                remaining_needed = top_k - len(processed_results)

                if remaining_needed > 0:
                    logger.info(f"üîç Buscando {remaining_needed} resultados adicionales en todos los chunks...")

                    # Buscar en todos (sin filtro de calidad)
                    all_results = self.collection.query(
                        query_texts=[query],
                        n_results=min(top_k * 3, 30),  # Buscar m√°s para filtrar duplicados
                        where=base_filter if base_filter else None,
                        include=["documents", "metadatas", "distances"]
                    )

                    # Agregar resultados no duplicados
                    existing_ids = set()
                    for result in processed_results:
                        # Usar hash del texto para evitar duplicados
                        existing_ids.add(hash(result["text"]))

                    if all_results["documents"] and all_results["documents"][0]:
                        for i in range(len(all_results["documents"][0])):
                            if len(processed_results) >= top_k:
                                break

                            text = all_results["documents"][0][i]
                            text_hash = hash(text)

                            # Evitar duplicados
                            if text_hash not in existing_ids:
                                metadata = all_results["metadatas"][0][i]
                                quality = metadata.get("chunk_quality", "sin_clasificar")

                                # Marcar prioridad seg√∫n calidad
                                if quality == "relevante":
                                    priority = "üéØ RELEVANTE"
                                elif quality == "ambiguo":
                                    priority = "üü° AMBIGUO"
                                elif quality == "irrelevante":
                                    priority = "üî¥ IRRELEVANTE"
                                else:
                                    priority = "‚ö™ SIN_CLASIFICAR"

                                result = {
                                    "text": text,
                                    "metadata": metadata,
                                    "distance": all_results["distances"][0][i],
                                    "similarity_score": 1 - all_results["distances"][0][i],
                                    "ranking": len(processed_results) + 1,
                                    "priority": priority
                                }
                                processed_results.append(result)
                                existing_ids.add(text_hash)

            else:
                # B√∫squeda normal sin priorizaci√≥n
                logger.info("üîç Buscando en todos los chunks...")
                results = self.collection.query(
                    query_texts=[query],
                    n_results=top_k,
                    where=base_filter if base_filter else None,
                    include=["documents", "metadatas", "distances"]
                )

                if results["documents"] and results["documents"][0]:
                    for i in range(len(results["documents"][0])):
                        metadata = results["metadatas"][0][i]
                        quality = metadata.get("chunk_quality", "sin_clasificar")

                        if quality == "relevante":
                            priority = "üéØ RELEVANTE"
                        elif quality == "ambiguo":
                            priority = "üü° AMBIGUO"
                        elif quality == "irrelevante":
                            priority = "üî¥ IRRELEVANTE"
                        else:
                            priority = "‚ö™ SIN_CLASIFICAR"

                        result = {
                            "text": results["documents"][0][i],
                            "metadata": metadata,
                            "distance": results["distances"][0][i],
                            "similarity_score": 1 - results["distances"][0][i],
                            "ranking": i + 1,
                            "priority": priority
                        }
                        processed_results.append(result)

            search_time = time.time() - start_time

            # Limitar a top_k resultados finales
            final_results = processed_results[:top_k]

            # Actualizar rankings finales
            for i, result in enumerate(final_results):
                result["ranking"] = i + 1

            logger.info(f"‚úÖ B√∫squeda completada en {search_time:.3f}s: {len(final_results)} resultados")

            # Log de distribuci√≥n de calidad
            quality_count = {}
            for result in final_results:
                priority = result["priority"]
                quality_count[priority] = quality_count.get(priority, 0) + 1

            if quality_count:
                logger.info("üìä Distribuci√≥n de resultados por calidad:")
                for priority, count in quality_count.items():
                    logger.info(f"   {priority}: {count} resultados")

            return final_results

        except Exception as e:
            logger.error(f"‚ùå Error en b√∫squeda: {e}")
            return []

    def get_collection_stats(self) -> Dict[str, Any]:
        """Obtiene estad√≠sticas de la colecci√≥n"""
        if not self.collection:
            return {"error": "No connection to Chroma"}

        try:
            count = self.collection.count()

            # Obtener muestra para an√°lisis
            sample = self.collection.get(limit=10, include=["metadatas"])

            # Analizar marcas
            brands = {}
            if sample["metadatas"]:
                for metadata in sample["metadatas"]:
                    brand = metadata.get("brand", "unknown")
                    brands[brand] = brands.get(brand, 0) + 1

            return {
                "total_documents": count,
                "collection_name": self.collection_name,
                "brands_sample": brands,
                "status": "connected"
            }

        except Exception as e:
            logger.error(f"Error obteniendo estad√≠sticas: {e}")
            return {"error": str(e)}

    def clear_collection(self):
        """Limpia toda la colecci√≥n (usar con cuidado)"""
        logger.warning("‚ö†Ô∏è Limpiando toda la colecci√≥n...")

        if self.collection:
            # Nota: Chroma no tiene clear() directo, necesitamos recrear
            self.client.delete_collection(name=self.collection_name)
            self._setup_collection()
            logger.info("üóëÔ∏è Colecci√≥n limpiada y recreada")

    def health_check(self) -> bool:
        """Verifica que Chroma DB est√© funcionando correctamente"""
        try:
            heartbeat = self.client.heartbeat()
            collection_count = self.collection.count()

            logger.info(f"üíö Health Check OK:")
            logger.info(f"   üîå Heartbeat: {heartbeat}")
            logger.info(f"   üìö Documentos: {collection_count}")

            return True

        except Exception as e:
            logger.error(f"üíî Health Check FAILED: {e}")
            return False


def test_chroma_connection():
    """Funci√≥n de prueba para verificar conexi√≥n con Chroma"""
    logger.info("üß™ Probando conexi√≥n con Chroma DB...")

    try:
        # Crear manager
        manager = ChromaManager()

        # Health check
        if manager.health_check():
            logger.info("‚úÖ Chroma DB funcionando correctamente")

            # Mostrar estad√≠sticas
            stats = manager.get_collection_stats()
            logger.info(f"üìä Estad√≠sticas: {stats}")

            return manager
        else:
            logger.error("‚ùå Chroma DB no est√° funcionando")
            return None

    except Exception as e:
        logger.error(f"üí• Error en test de conexi√≥n: {e}")
        return None


if __name__ == "__main__":
    # Test de conexi√≥n independiente
    import sys
    from loguru import logger

    logger.remove()
    logger.add(sys.stdout, level="INFO")

    manager = test_chroma_connection()